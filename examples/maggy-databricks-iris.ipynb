{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Maggy  is an open-source framework that simplifies writing and maintaining distributed machine learning programs.\n",
    "By encapsulating your training logic in a function,\n",
    "the same code can be run unchanged with Python on your laptop or distributed using PySpark for hyperparameter tuning, \n",
    "data-parallel training, or model-parallel training. \n",
    "With the arrival of GPU support in Spark 3.0, \n",
    "PySpark can be now used to orchestrate distributed deep learning applications in TensorFlow and PySpark.  \n",
    "We are pleased to announce we have now added support for Maggy on Databricks, \n",
    "so training machine learning models with many workers should be as easy as running Python programs on your laptop. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 0. Spark Session\n",
    "\n",
    "First, make sure you have a running Spark Session/Context available."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make sure you have the right tensorflow version."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%pip install tensorflow-cpu==2.4.1\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Model definition\n",
    "\n",
    "Let's define the model we want to train. The layers of the model have to be defined in the \\_\\_init__ function.\n",
    "\n",
    "Do not instantiate the class, otherwise you won't be able to use Maggy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# you can use keras.Sequential(), you just need to override it \n",
    "# on a custom class and define the layers in __init__()\n",
    "class NeuralNetwork(Sequential):\n",
    "        \n",
    "    def __init__(self, nl=4):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.add(Dense(10,input_shape=(None,4),activation='tanh'))\n",
    "        if nl >= 4:\n",
    "          for i in range(0, nl-2):\n",
    "            self.add(Dense(8,activation='tanh'))\n",
    "        self.add(Dense(3,activation='softmax'))\n",
    "\n",
    "model = NeuralNetwork"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Dataset creation\n",
    "\n",
    "In this example, we are using the iris dataset. Let's download the dataset from https://www.kaggle.com/uciml/iris and upload it on your Databricks profile.\n",
    "\n",
    "You can process the dataset in the notebook and pass it to the configuration classes, or process it during the experiment.\n",
    "In order to do that you have to wrap the processing logic in a function and pass it to the training configuration (this step is currently supported only by TfDistributedConfig).\n",
    "\n",
    "You need to change the dataset path is correct."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/FileStore/tables/iris_train-2.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_set_path = \"dbfs:/FileStore/tables/iris_train-2.csv\"\n",
    "test_set_path = \"dbfs:/FileStore/tables/iris_test-1.csv\"\n",
    "\n",
    "train_set = spark.read.format(\"csv\").option(\"header\",\"true\")\\\n",
    "  .option(\"inferSchema\", \"true\").load(train_set_path).drop('_c0')\n",
    "\n",
    "test_set = spark.read.format(\"csv\").option(\"header\",\"true\")\\\n",
    "  .option(\"inferSchema\", \"true\").load(test_set_path).drop('_c0')\n",
    "\n",
    "raw_train_set = train_set.toPandas().values\n",
    "raw_test_set = test_set.toPandas().values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also wrap the data processing in a function and pass it to the training configuration, as we'll see later."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_data(train_set, test_set):\n",
    "    \n",
    "    X_train = train_set[:,0:4]\n",
    "    y_train = train_set[:,4:]\n",
    "    X_test = test_set[:,0:4]\n",
    "    y_test = test_set[:,4:]\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "  \n",
    "train_set, test_set = process_data(raw_train_set, raw_test_set)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Wrap the training logics.\n",
    "\n",
    "The programming model is that you wrap the code containing the logics of your experiment in a function.\n",
    "\n",
    "For HPO, we have to define a function that has the HPs to be optimized as parameters. Inside the function we simply put\n",
    "the training logic as we were training our model in a single machine using Tensorflow."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def hpo_function(number_layers, reporter):\n",
    "  \n",
    "  model = NeuralNetwork(nl=number_layers)\n",
    "  model.build()\n",
    "  \n",
    "  #fitting the model and predicting\n",
    "  model.compile(Adam(lr=0.04),'categorical_crossentropy',metrics=['accuracy'])\n",
    "  train_input, test_input = process_data(raw_train_set, raw_test_set)\n",
    "\n",
    "  train_batch_size = 75\n",
    "  test_batch_size = 15\n",
    "  epochs = 10\n",
    "  \n",
    "  model.fit(x=train_input[0], y=train_input[1],\n",
    "            batch_size=train_batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=1)\n",
    "\n",
    "  score = model.evaluate(x=test_input[0], y=test_input[1], batch_size=test_batch_size, verbose=1)\n",
    "                         \n",
    "  print(f'Test loss: {score[0]}')\n",
    "  print(f'Test accuracy: {score[1]}')\n",
    "\n",
    "  return score[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We do the same for the training function, this time passing the model, train_set, test_set and hparams."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def training_function(model, train_set, test_set, hparams):\n",
    "    \n",
    "    model = model(nl=hparams['number_layers'])\n",
    "    model.build()\n",
    "    #fitting the model and predicting\n",
    "\n",
    "    model.compile(Adam(lr=hparams['learning_rate']),'categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    #raise ValueError(list(train_set.as_numpy_iterator()))\n",
    "\n",
    "    model.fit(train_set,epochs=hparams['epochs'])\n",
    "\n",
    "    accuracy = model.evaluate(test_set)\n",
    "\n",
    "    return accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the next step we have to create a configuration instance for Maggy. Since in this example we are using Maggy for hyperparameter optimization and distributed training using TensorFlow, we will use OptimizationConfig and TfDistributedConfig.\n",
    "\n",
    "### 4. Configure and run distributed HPO\n",
    "\n",
    "\n",
    "OptimizationConfig contains the information about the hyperparameter optimization.\n",
    "We need to define a Searchspace class that contains the hyperparameters we want to get optimized. In this example we want to search for the optimal number of layers of the neural network from 2 to 8 layers.\n",
    "\n",
    "OptimizationConfig the following parameters:\n",
    "* num_trials: Controls how many separate runs are conducted during the hp search.\n",
    "* optimizer: Optimizer type for searching the hp searchspace.\n",
    "* searchspace: A Searchspace object configuring the names, types and ranges of hps.\n",
    "* optimization_key: Name of the metric to use for hp search evaluation.\n",
    "* direction: Direction of optimization.\n",
    "* es_interval: Early stopping polling frequency during an experiment run.\n",
    "* es_min: Minimum number of experiments to conduct before starting the early stopping mechanism. Useful to establish a baseline for performance estimates.\n",
    "* es_policy: Early stopping policy which formulates a rule for triggering aborts.\n",
    "* name: Experiment name.\n",
    "* description: A description of the experiment.\n",
    "* hb_interval: Heartbeat interval with which the server is polling.\n",
    "* fixed_hp: Hyperparamets not to be tuned."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from maggy.experiment_config import OptimizationConfig\n",
    "from maggy import Searchspace\n",
    "\n",
    "# The searchspace can be instantiated with parameters\n",
    "sp = Searchspace(number_layers=('INTEGER', [2, 8]))\n",
    "\n",
    "hpo_config = OptimizationConfig(num_trials=4, optimizer=\"randomsearch\", searchspace=sp, direction=\"max\", es_interval=1, es_min=5, name=\"hp_tuning_test\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our HPO function and configuration class are now ready, so we can go on and run the HPO experiment. In order to do that, we run the lagom function, passing our training function and the configuration object we instantiated during the last step.\n",
    "Lagom is a swedish word meaning \"just the right amount\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from maggy import experiment\n",
    "\n",
    "result = experiment.lagom(train_fn=hpo_function, config=hpo_config)\n",
    "\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. Configure and run distributed training\n",
    "\n",
    "\n",
    "Now it's time to run the final step of our ML program. Let's initialize the configuration class for the distributed training. First, we need to define our hyperparameters, we want to take the best hyperparameters from the HPO."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#define the constructor parameters of your model\n",
    "model_params = {\n",
    "    #train dataset entries / num_workers\n",
    "    'train_batch_size': 75,\n",
    "    #test dataset entries / num_workers\n",
    "    'test_batch_size': 15,\n",
    "    'learning_rate': 0.04,\n",
    "    'epochs': 20,\n",
    "    'number_layers': result['best_config']['number_layers'],\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TfDistributedConfig class contains the following parameters:\n",
    "* name: the name of the experiment.\n",
    "* module: the model to be trained (defined in the first step of this guideline).\n",
    "* train_set: the train set as a tuple (x_train, y_train) or the train set path.\n",
    "* test_set: the test set as a tuple (x_test, y_test) or the test set path.\n",
    "* process_data: the function to process the data (if needed).\n",
    "* hparams: the model and dataset parameters. In this case we also need to provide the 'train_batch_size' and the 'test_batch_size', these values represent the subset sizes of the sharded dataset. It's value is usually the dataset_size/number_workers but can change depending on your needs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from maggy.experiment_config.tf_distributed import TfDistributedConfig\n",
    "\n",
    "training_config = TfDistributedConfig(name=\"tf_test\", model=model, train_set=train_set, test_set=test_set, process_data=process_data, hparams = model_params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we are ready to launch the maggy experiment. You just need to pass 2 parameters: the training function and the configuration variable we defined in the previous steps."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiment.lagom(training_function, training_config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  },
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "maggy-tf-dist-iris",
   "dashboards": [],
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "language": "python",
   "widgets": {},
   "notebookOrigID": 1415393082320837
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
